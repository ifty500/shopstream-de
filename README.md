# ShopStream DE â€” Modern Local Data Engineering Environment

A production-grade local data engineering environment that demonstrates modern data practices and tools.

## ğŸ›  Tech Stack

- **Orchestration:** Prefect 2
- **Storage:** PostgreSQL
- **Transformation:** dbt Core
- **Data Quality:** Great Expectations
- **Visualization:** Metabase
- **Infrastructure:** Docker Compose
- **Source:** DummyJSON API (users, orders, products) â€“ no authentication required

## ğŸš€ Quickstart

1. Clone and setup environment:
```bash
# Clone repository
git clone [repo-url]
cd shopstream-de

# Setup environment
cp .env.example .env        # Copy environment template
```

2. Start the services:
```bash
docker compose up -d        # Launch PostgreSQL, Metabase, and application
```

3. Run the pipeline:
```bash
make ingest                 # Full pipeline: extract â†’ validate â†’ transform
make dbt                    # Optional: Run dbt transforms separately
```

## ğŸ”Œ Service Access

- **Metabase:** [http://localhost:3000](http://localhost:3000)
  - First time setup required
  - Use database credentials from `.env`

- **PostgreSQL:**
  - Host: localhost
  - Port: 5437 (external)
  - Internal host: postgres (for services within Docker network)
  - Credentials: See `.env` file

## ğŸ“Š Data Pipeline

1. **Extraction:** 
   - Sources data from DummyJSON API
   - Loads users, products, and orders into `raw.*` schema

2. **Validation:**
   - Implements Great Expectations checks
   - Fails fast if data quality issues detected
   - Configurable validation rules

3. **Transformation:**
   - Uses dbt for data modeling
   - Creates a star schema:
     - Dimension tables: `dim_users`, `dim_products`, `dim_date`
     - Fact tables: `fact_orders`, `fact_order_items`
   - Implements staging layer (`stg.*`) for data cleaning

## âš™ï¸ Available Commands

```bash
make up         # Start all services
make down       # Stop services and remove volumes
make ingest     # Run complete Prefect pipeline
make dbt        # Execute dbt transformations and tests
```

## ğŸ“ Project Structure

```
shopstream-de/
â”œâ”€â”€ dbt_shopstream/           # DBT project
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”‚   â”œâ”€â”€ marts/          # Business-level tables
â”‚   â”‚   â””â”€â”€ staging/        # Cleaned raw data
â”‚   â””â”€â”€ tests/              # Data tests
â”œâ”€â”€ great_expectations/      # Data quality configs
â”œâ”€â”€ infra/                   # Infrastructure
â”‚   â””â”€â”€ postgres/           # Database initialization
â”œâ”€â”€ orchestration/          # Prefect flows
â”‚   â””â”€â”€ flows/             # Pipeline definitions
â””â”€â”€ pipelines/             # Core pipeline code
    â”œâ”€â”€ extract/          # Data extraction
    â”œâ”€â”€ load/            # Data loading
    â””â”€â”€ quality/         # Quality checks
```

## ğŸ”’ Security Notes

- Sensitive information is managed through environment variables
- Default credentials are for development only
- Production deployments should use secure credentials
- `.env` file is excluded from version control

## ğŸ’¡ Advanced Features

- **Idempotent Operations:** Safe to re-run pipelines
- **Error Handling:** Retries and failure management
- **Type Safety:** Strongly typed database schemas
- **Testing:** Comprehensive dbt and data quality tests
- **Monitoring:** Pipeline observability with Prefect

## ğŸš§ Future Improvements

- DuckDB support for local development
- Extended data quality checks
- Additional data sources
- Enhanced testing coverage
- Metrics and monitoring dashboards
